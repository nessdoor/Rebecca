#+title: Rebecca
#+subtitle: An augmented neural conversational agent

Rebecca is a general-purpose, extensible conversational agent built on top of a
large language model (LLM) for human-like text parsing and generation, supported
by natural-language processing technologies for reliable automated information
extraction, storage and learning.

* Roadmap
What follows is a list of current grand goals for the project, meant as a source
of inspiration for contributors willing to start tackling some new challenge.

** Short/mid-term memory
Neural language models are limited by the amount of text they can process at any
given time. Usually, an approximation of ultra-short-term memory is achieved by
supplying the model with a short record of previously sent messages (a
/context/), so that it can extract the long-distance relations between previous
and latter messages and give the illusion of an ongoing conversation[fn:1].

The contextual memory of a conversational agent can be extended by coupling the
completion generation part with some sort of information retrieval (IR)
system. At the moment, the authors are aware of two large classes of IR systems:
those based on "classical" IR studies (e.g. term-based indexing, probabilistic
document retrieval and ranking), and those relying on /text embeddings/
(i.e. vector representation of arbitrary text in a hyper-dimensional space, with
a notion of similarity/relatedness between different pieces of text). Given the
rise of neural language models, a wide variety of /vector databases/ and neural
information retrieval frameworks has appeared [fn:2].

A short-term goal for Rebecca is that of incorporating an IR system for
extending its memory, so that it can try to retrieve any missing information
from its stored experience, and dynamically generate an adequate context to be
submitted for completion generation.

A candidate for experimentation is [[https://lucene.apache.org/][Apache Lucene]], an established framework for
unstructured text querying. Since some minor versions ago, it has acquired the
ability to store dense floating-point vectors alongside each indexed document,
and then perform similarity queries on them. It seems that these new
functionalities can be seamlessly integrated with classical term-based queries,
opening a path for effective augmentation. The fact that it can run inside the
same runtime as that of core Rebecca is an added bonus. Its performance for our
usage scenario, though, remains to be proven.

An important limitation of embeddings is that of being tied to the model that
generated them. Different models can generate different embeddings, even of
different dimensionality. This means that we must either embed everything with a
free and open model, or we must maintain different embeddings in our store,
maybe dynamically re-embedding text on demand.

** Long-term concept learning and logical reasoning
Although it is possible to arbitrarily extend the memory of a language model by
storing all of its interactions for later retrieval (see the [[*Short/mid-term memory][relevant section]]),
it is still challenging to create comprehensive contexts from memorized text
alone. Even if remembered text can be automatically summarized before inclusion
(potentially multiplying the number of calls to external models, in the
process), the approach seems difficult to generalize to any piece of
information.

Another approach would be that of progressively creating a mechanized knowledge
base (KB) of all general knowledge that the agent possesses or learns throughout
its operational life. This would also help in solving the problem of information
accuracy for text generated by LLMs, as the agent can be primed with an
encyclopedic body of mechanized knowledge before deployment[fn:3]. In addition,
extensive research has been performed on ways to perform mechanical reasoning on
such KBs. Integrating a neural conversational agent with an automatic /reasoner/
and an appropriate on-line information extraction engine can allow the system to
generate contexts that hint at a logically-consistent conclusion backed by known
facts.

Finally, the ability to extract and learn concepts is an important contribution
to the creation of a long-term memory for the agent, as raw text that has been
harvested for information can be /forgotten/.

For the benefit of interoperability, relative maturity and academic interest,
the authors suggest to investigate [[https://www.w3.org/TR/rdf11-concepts/][RDF]] graphs for knowledge representation, [[https://www.w3.org/TR/owl2-primer/][OWL2]]
for ontological construction and OWL reasoners for inference. There are many
technological solutions to be explored in this space. As for information
extraction, some LLMs already seem to be proficient at performing this task, but
any output produced by them needs to be carefully parsed, normalized and,
occasionally, corrected. On the other hand, no particular difficulties have been
encountered in converting formalized knowledge (e.g. RDF stars in [[https://www.w3.org/TR/turtle/][Turtle syntax]])
back to natural-looking text.

** Topic tracking and partner identification
As of now, Rebecca has no hard concept of topics or threads of communication: it
communicates with all its conversation partners as if they belonged to a large
group. Any message is regarded as being sent into the same group chat, and
directly addressed at Rebecca itself. This not only creates problems of
confidentiality and appropriateness of responses, but it can also lead to
imprecise answers and confusion as to who is speaking to whom. The current
stopgap measure is to make the agent respond to only messages that are willingly
and directly addressed to it.

A good conversational agent suitable for group communication should be able to
identify topics that people are talking about, isolate the threads of concurrent
communication that are being carried out, and descend into the right context
when a response is requested inside one of these threads. In addition, the
system should generate each context taking into account the general knowledge of
the system, which could come from distinct threads.

This matter calls for a dynamic, fuzzy topic and thread classifier for messages
with /retractable decisions/, so that user feedback can be used to re-adjust
classification on-line. Solving this problem would also entail solutions to many
related problems, like the division of public and private conversation, and of
public conversations with different, potentially overlapping groups of people,
which conflict with the need of maintaining a global world knowledge in all
cases.

A possible implementation is still under investigation.

** Spontaneous interactions
Currently, Rebecca never initiates a conversation, and only acts in response to
a query coming from a conversation partner.

Conversations can be spontaneously initiated either on a random basis, when a
certain topic of interest is snooped from a conversation, or when a certain
change in the world state is detected.

A possible implementation is still under investigation.

** Mood and attitude modeling
LLM-based conversational agents are usually instructed on the tone to maintain
while responding, and always maintain the same attitude throughout the whole
conversation.

Mood can be modeled as a point in a multi-dimensional space, and an arbitrary
curve identifies the possible transitions within this space in response to some
external stimuli. This curve characterizes the agent in all non-verbal
dimensions of communication. Mood can be modeled as a non-linear, time-varying
dynamical system, where the dimensionality of the inputs and the transfer
function determine the sensibility of the agent. How interactions are
transformed into inputs to this system is still a matter of discussion. An
advanced implementation of the system would also account for a time-varying
transfer function.

A possible implementation is still under investigation.

** Generalized HCI connector
Up to now, Rebecca has only been briefly tested as a bot user on Telegram
chats. While the implementation of this interaction method needs further
refinement[fn:4], much greater engagement can be achieved by connecting the
agent to multiple communication platforms, preserving information and identities
across all of them. This can be supported by other advanced functionalities,
like [[*Long-term concept learning and logical reasoning][long-term concept learning]] (e.g. to link different online identities to the
same person, or to share learned truths among all conversations) and [[*Topic tracking and partner identification][topic
tracking]] (e.g. to carry out a complex conversation over multiple channels, with
different groups of users).

The means of human-machine communication should not be limited to the textual
media, but should also take into account things like voice communication or
information embedded in images.

** Modeling others
One of the characteristics of intelligent beings is the ability of predicting
actions and reactions, and making choices based on these.

With LLMs and other large neural models, it has become possible to construct
simulacra of real and imaginary people (up to a certain degree of
fidelity)[fn:5]. After having interacted with a person for some time, an
intelligent agent might be able to create a model for them by feeding all their
responses to a model tuner. The resulting model can then be consulted by the
agent whenever a communication choice has to be made, and selecting the most
appropriate action relative to some end goal. When multiple people are involved
in a conversation, their models can be networked into a model for their group.

This item is an ambitious goal that the authors deemed worth of being discussed,
but no actual investigation into the necessary technological solutions and
implementation has been carried out, yet.

* Footnotes

[fn:1] Current neural language models live an eternal present. The entire
conversation and its premises are presented to them as an instant snapshot of
the current situation, and they provide the most likely extension to it.

[fn:2] Look at [[https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696][this]] and [[https://medium.com/@dmitry-kan/neural-search-frameworks-a-head-to-head-comparison-976aa6662d20][this]] articles for an example of some vector DBs and
neural IR systems.

[fn:3] Take, for example, [[dbpedia.org][DBpedia]] or [[www.wikidata.org][WikiData]].

[fn:4] The Telegram connector has been hastily implemented as part of a sprint
to get the first real-world feedback as soon as possible.

[fn:5] Neural models are generic mathematical models capable of universal
approximation learned by example.
